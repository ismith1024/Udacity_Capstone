{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and Global Variables (make sure to run this cell)  { display-mode: \"form\" }\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
    "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(warning_status)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
    "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
    "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "from IPython.core.pylabtools import figsize\n",
    "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
    "%config InlineBackend.figure_format = notebook_screen_res\n",
    "\n",
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "# Eager Execution\n",
    "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
    "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
    "use_tf_eager = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Use try/except so we can easily re-execute the whole notebook.\n",
    "if use_tf_eager:\n",
    "    try:\n",
    "        tf.enable_eager_execution()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "  \n",
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    " \n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "TFColor = _TFColor()\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=True, enable_xla=False):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "    \n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.log_device_placement = True\n",
    "    if enable_gpu_ram_resizing:\n",
    "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "        config.gpu_options.allow_growth = True\n",
    "    if enable_xla:\n",
    "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
    "        config.graph_options.optimizer_options.global_jit_level = (\n",
    "            tf.OptimizerOptions.ON_1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reset_sess(config=None):\n",
    "    \"\"\"\n",
    "    Convenience function to create the TF graph & session or reset them.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = session_options(enable_gpu_ram_resizing=True, enable_xla=False)\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "reset_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Run-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Data and assumptions\n",
    "\n",
    "count_data = tf.constant(transcript_series, dtype=tf.float32)\n",
    "n_count_data = tf.shape(count_data)\n",
    "days = tf.range(n_count_data[0])\n",
    "\n",
    "# Convert from TF to numpy.\n",
    "\n",
    "[\n",
    "    count_data_, \n",
    "    n_count_data_, \n",
    "    days_,\n",
    "] = evaluate([\n",
    "    count_data, \n",
    "    n_count_data,\n",
    "    days,\n",
    "])\n",
    "\n",
    "# Visualizing the Results\n",
    "    \n",
    "plt.figure(figsize=(12.5, 4))\n",
    "plt.bar(days_, count_data_, color=\"#5DA5DA\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"count of sales made\")\n",
    "plt.title(\"Did the sales change over time?\")\n",
    "plt.xlim(0, n_count_data_[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(count_data, lambda_1, lambda_2, tau):\n",
    "    tfd = tfp.distributions\n",
    " \n",
    "    alpha = np.array(1. / count_data.mean(), np.float32)\n",
    "    rv_lambda_1 = tfd.Exponential(rate=alpha)\n",
    "    rv_lambda_2 = tfd.Exponential(rate=alpha)\n",
    " \n",
    "    rv_tau = tfd.Uniform()\n",
    " \n",
    "    lambda_ = tf.gather(\n",
    "         [lambda_1, lambda_2],\n",
    "         indices=tf.to_int32(tau * count_data.size <= np.arange(count_data.size)))\n",
    "    rv_observation = tfd.Poisson(rate=lambda_)\n",
    " \n",
    "    return (\n",
    "         rv_lambda_1.log_prob(lambda_1)\n",
    "         + rv_lambda_2.log_prob(lambda_2)\n",
    "         + rv_tau.log_prob(tau)\n",
    "         + tf.reduce_sum(rv_observation.log_prob(count_data))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda1\"),\n",
    "    tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda2\"),\n",
    "    0.5 * tf.ones([], dtype=tf.float32, name=\"init_tau\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.  \n",
    "]\n",
    "\n",
    "\n",
    "def joint_log_prob(count_data, lambda_1, lambda_2, tau):\n",
    "    tfd = tfp.distributions\n",
    " \n",
    "    alpha = (1. / tf.reduce_mean(count_data))\n",
    "    rv_lambda_1 = tfd.Exponential(rate=alpha)\n",
    "    rv_lambda_2 = tfd.Exponential(rate=alpha)\n",
    " \n",
    "    rv_tau = tfd.Uniform()\n",
    " \n",
    "\n",
    "    lambda_ = tf.gather(\n",
    "         [lambda_1, lambda_2],\n",
    "         indices=tf.to_int32(tau * tf.to_float(tf.size(count_data)) <= tf.to_float(tf.range(tf.size(count_data)))))\n",
    "    rv_observation = tfd.Poisson(rate=lambda_)\n",
    " \n",
    "    return (\n",
    "         rv_lambda_1.log_prob(lambda_1)\n",
    "         + rv_lambda_2.log_prob(lambda_2)\n",
    "         + rv_tau.log_prob(tau)\n",
    "         + tf.reduce_sum(rv_observation.log_prob(count_data))\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "def unnormalized_log_posterior(lambda1, lambda2, tau):\n",
    "    return joint_log_prob(count_data, lambda1, lambda2, tau)\n",
    "\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):#reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant(0.05, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    )\n",
    "    \n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "# Sample from the chain.\n",
    "[\n",
    "    lambda_1_samples,\n",
    "    lambda_2_samples,\n",
    "    posterior_tau,\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=100000,\n",
    "    num_burnin_steps=10000,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=unnormalized_log_posterior,\n",
    "            num_leapfrog_steps=2,\n",
    "            step_size=step_size,\n",
    "            step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(10000), #was ()\n",
    "            state_gradients_are_stopped=True),\n",
    "        bijector=unconstraining_bijectors))\n",
    "\n",
    "tau_samples = tf.floor(posterior_tau * tf.to_float(tf.size(count_data)))\n",
    "\n",
    "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "# N samples from the corresponding posterior distribution\n",
    "N = tf.shape(tau_samples)[0]\n",
    "expected_texts_per_day = tf.zeros(n_count_data)\n",
    "\n",
    "\n",
    "# Initialize any created variables.\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    lambda_1_samples_,\n",
    "    lambda_2_samples_,\n",
    "    tau_samples_,\n",
    "    kernel_results_,\n",
    "    N_,\n",
    "    expected_texts_per_day_,\n",
    "] = evaluate([\n",
    "    lambda_1_samples,\n",
    "    lambda_2_samples,\n",
    "    tau_samples,\n",
    "    kernel_results,\n",
    "    N,\n",
    "    expected_texts_per_day,\n",
    "])\n",
    "\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "print(\"final step size: {}\".format(\n",
    "    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.5, 15))\n",
    "#histogram of the samples:\n",
    "\n",
    "ax = plt.subplot(311)\n",
    "ax.set_autoscaley_on(False)\n",
    "\n",
    "plt.hist(lambda_1_samples_, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=r\"posterior of $\\lambda_1$\", color=TFColor[0], density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(r\"\"\"Posterior distributions of the variables $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "#plt.xlim([0, 50])\n",
    "plt.xlabel(r\"$\\lambda_1$ value\")\n",
    "\n",
    "ax = plt.subplot(312)\n",
    "ax.set_autoscaley_on(False)\n",
    "plt.hist(lambda_2_samples_, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=r\"posterior of $\\lambda_2$\", color=TFColor[6], density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.xlim([0, 50])\n",
    "plt.xlabel(r\"$\\lambda_2$ value\")\n",
    "\n",
    "plt.subplot(313)\n",
    "w = 1.0 / tau_samples_.shape[0] * np.ones_like(tau_samples_)\n",
    "plt.hist(tau_samples_, bins=n_count_data_[0], alpha=1,\n",
    "         label=r\"posterior of $\\tau$\",\n",
    "         color=TFColor[2], weights=w, rwidth=2.)\n",
    "plt.xticks(np.arange(n_count_data_[0]))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, .75])\n",
    "#plt.xlim([0, len(count_data_)-20])\n",
    "plt.xlabel(r\"$\\tau$ (in days)\")\n",
    "plt.ylabel(r\"probability\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.5, 9))\n",
    "\n",
    "for day in range(0, n_count_data_[0]):\n",
    "    # ix is a bool index of all tau samples corresponding to\n",
    "    # the switchpoint occurring prior to value of 'day'\n",
    "    ix = day < tau_samples_\n",
    "    # Each posterior sample corresponds to a value for tau.\n",
    "    # for each day, that value of tau indicates whether we're \"before\"\n",
    "    # (in the lambda1 \"regime\") or\n",
    "    #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
    "    # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
    "    # over all samples to get an expected value for lambda on that day.\n",
    "    # As explained, the \"message count\" random variable is Poisson distributed,\n",
    "    # and therefore lambda (the poisson parameter) is the expected value of\n",
    "    # \"message count\".\n",
    "    expected_texts_per_day_[day] = (lambda_1_samples_[ix].sum()\n",
    "                                   + lambda_2_samples_[~ix].sum()) / N_\n",
    "\n",
    "\n",
    "plt.plot(range(n_count_data_[0]), expected_texts_per_day_, lw=4, color=\"#E24A33\",\n",
    "         label=\"expected number of sales made\")\n",
    "plt.xlim(0, n_count_data_[0])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Expected # sales\")\n",
    "plt.title(\"Expected number of sales made\")\n",
    "plt.ylim(0, 5000)\n",
    "plt.bar(np.arange(len(count_data_)), count_data_, color=\"#5DA5DA\", alpha=0.65,\n",
    "        label=\"observed sales per hour\")\n",
    "\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "EXAMPLE INITIALIZATION\n",
    "transactions = []\n",
    "\n",
    "offer_id = portfolio.loc[8, 'id']\n",
    "offer_duration_days = portfolio.loc[8, 'duration']\n",
    "offer_duration_hours = 24 * offer_duration_days\n",
    "\n",
    "#lookup the users who received the offer and the start time\n",
    "transcript[transcript['offer_id'] == str(offer_id)]\n",
    "\n",
    "demo_df = transcript[transcript['offer_id'] == offer_id]\n",
    "demo_df.apply(get_transacts, axis = 1)\n",
    "\n",
    "transaction_times = [i[0] for i in transactions]\n",
    "plt.hist(transaction_times, bins = 100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "def evaluate_bayesian(transaction_times):\n",
    "    #transaction history to time series\n",
    "    transcript_series = np.zeros(max(transaction_times) + 1)\n",
    "    for i in transaction_times:\n",
    "        transcript_series[i] = transcript_series[i] + 1\n",
    "\n",
    "\n",
    "        #Set up the markov chain\n",
    "        # Set the chain's start state.\n",
    "    initial_chain_state = [\n",
    "        tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda1\"),\n",
    "        tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda2\"),\n",
    "        0.5 * tf.ones([], dtype=tf.float32, name=\"init_tau\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Since HMC operates over unconstrained space, we need to transform the\n",
    "    # samples so they live in real-space.\n",
    "    unconstraining_bijectors = [\n",
    "        tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "        tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "        tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.  \n",
    "    ]                                                             \n",
    "\n",
    "\n",
    "    # Initialize the step_size. (It will be automatically adapted.)\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):#reuse=tf.AUTO_REUSE):\n",
    "        step_size = tf.get_variable(\n",
    "            name='step_size',\n",
    "            initializer=tf.constant(0.05, dtype=tf.float32),\n",
    "            trainable=False,\n",
    "            use_resource=True\n",
    "        )\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    # Sample from the chain.\n",
    "    [\n",
    "        lambda_1_samples,\n",
    "        lambda_2_samples,\n",
    "        posterior_tau,\n",
    "    ], kernel_results = tfp.mcmc.sample_chain(\n",
    "        num_results=100000,\n",
    "        num_burnin_steps=10000,\n",
    "        current_state=initial_chain_state,\n",
    "        kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "            inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                target_log_prob_fn=unnormalized_log_posterior,\n",
    "                num_leapfrog_steps=2,\n",
    "                step_size=step_size,\n",
    "                step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(10000), #was ()\n",
    "                state_gradients_are_stopped=True),\n",
    "            bijector=unconstraining_bijectors))\n",
    "\n",
    "    tau_samples = tf.floor(posterior_tau * tf.to_float(tf.size(count_data)))\n",
    "\n",
    "    # tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "    # N samples from the corresponding posterior distribution\n",
    "    N = tf.shape(tau_samples)[0]\n",
    "    expected_texts_per_day = tf.zeros(n_count_data)\n",
    "\n",
    "\n",
    "    # Initialize any created variables.\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    \n",
    "    evaluate(init_g)\n",
    "    evaluate(init_l)\n",
    "    [\n",
    "        lambda_1_samples_,\n",
    "        lambda_2_samples_,\n",
    "        tau_samples_,\n",
    "        kernel_results_,\n",
    "        N_,\n",
    "        expected_texts_per_day_,\n",
    "    ] = evaluate([\n",
    "        lambda_1_samples,\n",
    "        lambda_2_samples,\n",
    "        tau_samples,\n",
    "        kernel_results,\n",
    "        N,\n",
    "        expected_texts_per_day,\n",
    "    ])\n",
    "\n",
    "\n",
    "    print(\"acceptance rate: {}\".format(\n",
    "        kernel_results_.inner_results.is_accepted.mean()))\n",
    "    print(\"final step size: {}\".format(\n",
    "        kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n",
    "    \n",
    "    plt.figure(figsize=(12.5, 15))\n",
    "    #histogram of the samples:\n",
    "\n",
    "    ax = plt.subplot(311)\n",
    "    ax.set_autoscaley_on(False)\n",
    "\n",
    "    plt.hist(lambda_1_samples_, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "             label=r\"posterior of $\\lambda_1$\", color=TFColor[0], density=True)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(r\"\"\"Posterior distributions of the variables $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "    #plt.xlim([0, 50])\n",
    "    plt.xlabel(r\"$\\lambda_1$ value\")\n",
    "\n",
    "    ax = plt.subplot(312)\n",
    "    ax.set_autoscaley_on(False)\n",
    "    plt.hist(lambda_2_samples_, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "             label=r\"posterior of $\\lambda_2$\", color=TFColor[6], density=True)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    #plt.xlim([0, 50])\n",
    "    plt.xlabel(r\"$\\lambda_2$ value\")\n",
    "\n",
    "    plt.subplot(313)\n",
    "    w = 1.0 / tau_samples_.shape[0] * np.ones_like(tau_samples_)\n",
    "    plt.hist(tau_samples_, bins=n_count_data_[0], alpha=1,\n",
    "             label=r\"posterior of $\\tau$\",\n",
    "             color=TFColor[2], weights=w, rwidth=2.)\n",
    "    plt.xticks(np.arange(n_count_data_[0]))\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, .75])\n",
    "    #plt.xlim([0, len(count_data_)-20])\n",
    "    plt.xlabel(r\"$\\tau$ (in days)\")\n",
    "    plt.ylabel(r\"probability\");\n",
    "    \n",
    "    plt.figure(figsize=(12.5, 9))\n",
    "\n",
    "    for day in range(0, n_count_data_[0]):\n",
    "        # ix is a bool index of all tau samples corresponding to\n",
    "        # the switchpoint occurring prior to value of 'day'\n",
    "        ix = day < tau_samples_\n",
    "        # Each posterior sample corresponds to a value for tau.\n",
    "        # for each day, that value of tau indicates whether we're \"before\"\n",
    "        # (in the lambda1 \"regime\") or\n",
    "        #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
    "        # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
    "        # over all samples to get an expected value for lambda on that day.\n",
    "        # As explained, the \"message count\" random variable is Poisson distributed,\n",
    "        # and therefore lambda (the poisson parameter) is the expected value of\n",
    "        # \"message count\".\n",
    "        expected_texts_per_day_[day] = (lambda_1_samples_[ix].sum()\n",
    "                                       + lambda_2_samples_[~ix].sum()) / N_\n",
    "\n",
    "\n",
    "    plt.plot(range(n_count_data_[0]), expected_texts_per_day_, lw=4, color=\"#E24A33\",\n",
    "             label=\"expected number of sales made\")\n",
    "    plt.xlim(0, n_count_data_[0])\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Expected # sales\")\n",
    "    plt.title(\"Expected number of sales made\")\n",
    "    plt.ylim(0, 5000)\n",
    "    plt.bar(np.arange(len(count_data_)), count_data_, color=\"#5DA5DA\", alpha=0.65,\n",
    "            label=\"observed sales per hour\")\n",
    "\n",
    "    plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "\n",
    "user_query = list(profile[profile['gender'] == 'M']['id'])\n",
    "\n",
    "offer_id = portfolio.loc[8, 'id']\n",
    "offer_duration_days = portfolio.loc[8, 'duration']\n",
    "offer_duration_hours = 24 * offer_duration_days\n",
    "\n",
    "#lookup the users who received the offer and the start time\n",
    "#transcript[(transcript['offer_id'] == str(offer_id)) & (transcript['person'] in user_query) ]\n",
    "\n",
    "demo_df = transcript.query('offer_id == @offer_id & person in @user_query')\n",
    "demo_df.apply(get_transacts, axis = 1)\n",
    "\n",
    "transaction_times = [i[0] for i in transactions]\n",
    "plt.hist(transaction_times, bins = 100)\n",
    "\n",
    "evaluate_bayesian(transaction_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
